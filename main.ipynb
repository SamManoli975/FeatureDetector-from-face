{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting bleach (from kaggle)\n",
      "  Downloading bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting certifi>=14.05.14 (from kaggle)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting charset-normalizer (from kaggle)\n",
      "  Downloading charset_normalizer-3.4.1-cp38-cp38-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna (from kaggle)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting protobuf (from kaggle)\n",
      "  Downloading protobuf-5.29.4-cp38-cp38-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\samma\\anaconda3\\envs\\face_recog_env\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting requests (from kaggle)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\samma\\anaconda3\\envs\\face_recog_env\\lib\\site-packages (from kaggle) (75.1.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\samma\\anaconda3\\envs\\face_recog_env\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Collecting text-unidecode (from kaggle)\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tqdm (from kaggle)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting urllib3>=1.15.1 (from kaggle)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting webencodings (from kaggle)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\samma\\anaconda3\\envs\\face_recog_env\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Using cached kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading bleach-6.1.0-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp38-cp38-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading protobuf-5.29.4-cp38-cp38-win_amd64.whl (434 kB)\n",
      "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, text-unidecode, urllib3, tqdm, python-slugify, protobuf, idna, charset-normalizer, certifi, bleach, requests, kaggle\n",
      "Successfully installed bleach-6.1.0 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 kaggle-1.7.4.2 protobuf-5.29.4 python-slugify-8.0.4 requests-2.32.3 text-unidecode-1.3 tqdm-4.67.1 urllib3-2.2.3 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "import kaggle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/jangedoo/utkface-new\n",
      "License(s): copyright-authors\n"
     ]
    }
   ],
   "source": [
    "api.dataset_download_cli('jangedoo/utkface-new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('utkface-new.zip', 'r') as zipref:\n",
    "    zipref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:\\VS CODE\\ML\\utkface_aligned_cropped\\crop_part1\\61_1_20170109142408075.jpg.chip.jpg: invalid literal for int() with base 10: '20170109142408075.jpg.chip.jpg'\n",
      "Error processing C:\\VS CODE\\ML\\utkface_aligned_cropped\\crop_part1\\61_3_20170109150557335.jpg.chip.jpg: invalid literal for int() with base 10: '20170109150557335.jpg.chip.jpg'\n",
      "Loaded 9778 images with corresponding ethnicity labels.\n",
      "White: 5265 images\n",
      "Black: 405 images\n",
      "Asian: 1553 images\n",
      "Indian: 1452 images\n",
      "Others: 1103 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the cropped images\n",
    "data_dir = r\"C:\\VS CODE\\ML\\utkface_aligned_cropped\\crop_part1\"\n",
    "\n",
    "# Function to load and process images with ethnicity labels\n",
    "def load_images_from_folder(data_dir):\n",
    "    images = []\n",
    "    ethnicity_labels = []\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            \n",
    "            try:\n",
    "                # Load the image using OpenCV\n",
    "                image = cv2.imread(file_path)\n",
    "                if image is None:\n",
    "                    print(f\"Failed to load image: {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Extract the ethnicity (race) from the filename\n",
    "                # Format is typically [age]_[gender]_[race]_[date&time].jpg\n",
    "                parts = filename.split(\"_\")\n",
    "                if len(parts) >= 3:\n",
    "                    ethnicity = int(parts[2])  # Ethnicity is the third element\n",
    "                    \n",
    "                    # Append the image and ethnicity label to the lists\n",
    "                    images.append(image)\n",
    "                    ethnicity_labels.append(ethnicity)\n",
    "                else:\n",
    "                    print(f\"Filename format incorrect for {filename}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert lists to numpy arrays for ML models\n",
    "    images = np.array(images)\n",
    "    ethnicity_labels = np.array(ethnicity_labels)\n",
    "    \n",
    "    return images, ethnicity_labels\n",
    "\n",
    "# Load images and ethnicity labels\n",
    "images, ethnicity_labels = load_images_from_folder(data_dir)\n",
    "print(f\"Loaded {len(images)} images with corresponding ethnicity labels.\")\n",
    "\n",
    "# Optionally, create a mapping for better understanding\n",
    "ethnicity_mapping = {\n",
    "    0: \"White\",\n",
    "    1: \"Black\",\n",
    "    2: \"Asian\",\n",
    "    3: \"Indian\",\n",
    "    4: \"Others\"\n",
    "}\n",
    "\n",
    "# Print distribution of ethnicities\n",
    "for ethnicity_id, ethnicity_name in ethnicity_mapping.items():\n",
    "    count = np.sum(ethnicity_labels == ethnicity_id)\n",
    "    print(f\"{ethnicity_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4000 training samples and 1000 testing samples.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the path to the cropped images\n",
    "data_dir = r\"C:\\VS CODE\\ML\\utkface_aligned_cropped\\crop_part1\"\n",
    "\n",
    "# Function to load & process a smaller dataset\n",
    "def load_small_dataset(data_dir, max_images=5000, img_size=(64, 64)):  \n",
    "    images, labels = [], []\n",
    "    count = 0\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".jpg\") and count < max_images:\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            img = cv2.imread(file_path)\n",
    "            \n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.resize(img, img_size)  # Resize to 64x64\n",
    "            img = img / 255.0  # Normalize\n",
    "            \n",
    "            # Extract ethnicity from filename (format: age_gender_ethnicity_time.jpg)\n",
    "            parts = filename.split(\"_\")\n",
    "            if len(parts) >= 3:\n",
    "                try:\n",
    "                    ethnicity = int(parts[2])\n",
    "                    images.append(img)\n",
    "                    labels.append(ethnicity)\n",
    "                    count += 1\n",
    "                except ValueError:\n",
    "                    continue  # Skip invalid labels\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load reduced dataset\n",
    "images, labels = load_small_dataset(data_dir)\n",
    "\n",
    "# Split data into train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(labels))\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"Loaded {len(X_train)} training samples and {len(X_test)} testing samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1924 training samples and 481 testing samples.\n",
      "Class distribution in training set: {np.int64(0): 400, np.int64(4): 400, np.int64(2): 400, np.int64(3): 400, np.int64(1): 324}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "\n",
    "# Define the path to the cropped images\n",
    "data_dir = r\"C:\\VS CODE\\ML\\utkface_aligned_cropped\\crop_part1\"\n",
    "\n",
    "# Function to load & process dataset with under-sampling\n",
    "def load_and_under_sample_dataset(data_dir, max_images_per_class=500):  \n",
    "    images, labels = [], []\n",
    "    \n",
    "    # Dictionary to store image lists per ethnicity\n",
    "    ethnicity_dict = {0: [], 1: [], 2: [], 3: [], 4: []}  # 0: White, 1: Black, 2: Asian, 3: Indian, 4: Others\n",
    "\n",
    "    # Load images and categorize them by ethnicity\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            img = cv2.imread(file_path)\n",
    "            \n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            img = cv2.resize(img, (64, 64))  # Resize to 64x64\n",
    "            img = img / 255.0  # Normalize\n",
    "            \n",
    "            # Extract ethnicity from filename (format: age_gender_ethnicity_time.jpg)\n",
    "            parts = filename.split(\"_\")\n",
    "            if len(parts) >= 3:\n",
    "                try:\n",
    "                    ethnicity = int(parts[2])\n",
    "                    ethnicity_dict[ethnicity].append(img)  # Add image to the corresponding ethnicity list\n",
    "                except ValueError:\n",
    "                    continue  # Skip invalid labels\n",
    "    \n",
    "    # Now, under-sample each class to have a max of `max_images_per_class`\n",
    "    for ethnicity in ethnicity_dict:\n",
    "        if len(ethnicity_dict[ethnicity]) > max_images_per_class:\n",
    "            ethnicity_dict[ethnicity] = ethnicity_dict[ethnicity][:max_images_per_class]  # Limit the samples\n",
    "    \n",
    "    # Flatten lists of images and labels\n",
    "    for ethnicity in ethnicity_dict:\n",
    "        images.extend(ethnicity_dict[ethnicity])\n",
    "        labels.extend([ethnicity] * len(ethnicity_dict[ethnicity]))  # Repeat ethnicity label for each image\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load dataset with under-sampling\n",
    "images, labels = load_and_under_sample_dataset(data_dir)\n",
    "\n",
    "# Split data into train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(labels))\n",
    "y_train_categorical = to_categorical(y_train, num_classes)\n",
    "y_test_categorical = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"Loaded {len(X_train)} training samples and {len(X_test)} testing samples.\")\n",
    "print(f\"Class distribution in training set: {dict(Counter(y_train))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS CODE\\ML\\FeatureDetection\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m401,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">406,885</span> (1.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m406,885\u001b[0m (1.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">406,885</span> (1.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m406,885\u001b[0m (1.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Simple CNN Model\n",
    "def build_light_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')  # Multi-class classification\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get input shape (64x64x3 for RGB)\n",
    "input_shape = X_train.shape[1:]\n",
    "model = build_light_cnn(input_shape, num_classes)\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\VS CODE\\ML\\FeatureDetection\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - accuracy: 0.2185 - loss: 1.6084 - val_accuracy: 0.3721 - val_loss: 1.4982\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.3201 - loss: 1.5194 - val_accuracy: 0.4220 - val_loss: 1.4302\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.3480 - loss: 1.4768 - val_accuracy: 0.3514 - val_loss: 1.4782\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.3331 - loss: 1.4823 - val_accuracy: 0.4428 - val_loss: 1.3674\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.3712 - loss: 1.4756 - val_accuracy: 0.4200 - val_loss: 1.3766\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.3712 - loss: 1.4424 - val_accuracy: 0.4491 - val_loss: 1.3639\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.3940 - loss: 1.4122 - val_accuracy: 0.4241 - val_loss: 1.3713\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.3866 - loss: 1.4245 - val_accuracy: 0.4740 - val_loss: 1.3094\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.4088 - loss: 1.4081 - val_accuracy: 0.4595 - val_loss: 1.3208\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.4037 - loss: 1.3835 - val_accuracy: 0.4948 - val_loss: 1.2901\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Initialize ImageDataGenerator with augmentation for minority classes\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the data generator on your training data (X_train)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Now, train the model using the augmented data\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train_categorical, batch_size=32),\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test_categorical)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5234 - loss: 1.2703\n",
      "Test Accuracy: 49.48%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test_categorical)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('C:\\VS CODE\\ML\\Models\\ethnicity_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "The predicted ethnicity is: Black\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('C:\\VS CODE\\ML\\Models\\ethnicity_model.h5')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def predict_ethnicity(image_path):\n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Check if the image is successfully loaded\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read the image from path: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = cv2.resize(img, (64,64))  # Resize to match the input size for the model\n",
    "    img = img / 255.0  # Normalize the pixel values to [0, 1]\n",
    "    \n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension (shape becomes (1, 128, 128, 3))\n",
    "    \n",
    "    # Predict the ethnicity\n",
    "    predictions = model.predict(img)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]  # Get the class with the highest probability\n",
    "    \n",
    "    return ethnicity_mapping[predicted_class]\n",
    "\n",
    "# Test the function with a new image\n",
    "image_path = 'asianperson2.jpeg'  # Replace with the path to your image\n",
    "predicted_ethnicity = predict_ethnicity(image_path)\n",
    "print(f\"The predicted ethnicity is: {predicted_ethnicity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
